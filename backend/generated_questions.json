{
  "lecture_id": "L2001",
  "title": "Transformer Architecture for LLMs",
  "generated_questions": [
    {
      "question_id": 1,
      "difficulty": "easy",
      "question": "What type of mechanism are Transformer neural network architectures based on?",
      "answer": "Self-attention mechanisms.",
      "answer_type": "short_text"
    },
    {
      "question_id": 2,
      "difficulty": "easy",
      "question": "What structural element do Transformers replace recurrence with?",
      "answer": "Parallel attention layers.",
      "answer_type": "short_text"
    },
    {
      "question_id": 3,
      "difficulty": "medium",
      "question": "Name two of the main components included in the Transformer architecture.",
      "answer": "Any two of: embeddings, positional encodings, multi-head self-attention, feed-forward layers, residual connections, or layer normalization.",
      "answer_type": "short_text"
    },
    {
      "question_id": 4,
      "difficulty": "medium",
      "question": "What are the three specific elements used by the attention mechanism to compute weighted combinations of token representations?",
      "answer": "Queries, keys, and values.",
      "answer_type": "short_text"
    },
    {
      "question_id": 5,
      "difficulty": "medium",
      "question": "Besides self-attention, what other specific type of connection or layer is listed as a main component of the Transformer architecture?",
      "answer": "Residual connections or layer normalization.",
      "answer_type": "short_text"
    },
    {
      "question_id": 6,
      "difficulty": "hard",
      "question": "What specific operation does the attention mechanism perform using queries, keys, and values?",
      "answer": "It computes weighted combinations of token representations.",
      "answer_type": "short_text"
    }
  ]
}